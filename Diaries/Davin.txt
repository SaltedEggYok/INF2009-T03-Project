22nd Feb:
Researched on the potential methodology and modeles which can be utilised for the facial emotion recognition model. Originally started with the 7 classes. Started with the implementation of the model and implemented the Dataset class

29th Feb:
Created the generate_dataset script, creating the respective train test and val datasets for reproducibility.Implemented the model architecture and model wrapper and adapted training method from https://github.com/AmrElsersy/Emotions-Recognition. 

3rd March:
Finished implementation of base model training and evaluation. Changed the specific classes to generic positive negative and neutral classes to reduce number of possible output classes for prediction.

7th March:
Implemented inferrence on one image. Tested the model to check if emotion detector is working on real and unseen data. 

15th March:
Started work on post processing of outputs of the model classifier, retrieved outputs as a dictionary and thinking of format of data to be outputted using MQTT

22nd March:
Completed post processing of outputs, decided to send data in the form of 
(18, 31, 25): {0: 'Neutral'}
(18, 31, 27): {0: 'Neutral'}
(18, 31, 28): {0: 'Neutral', 1: 'Negative'}
where {time_stamp : {user:emotion}}
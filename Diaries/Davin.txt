22nd Feb:
Researched on the potential methodology and modeles which can be utilised for the facial emotion recognition model. Originally started with the 7 classes. Started with the implementation of the model and implemented the Dataset class

29th Feb:
Created the generate_dataset script, creating the respective train test and val datasets for reproducibility.Implemented the model architecture and model wrapper and adapted training method from https://github.com/AmrElsersy/Emotions-Recognition. 

3rd March:
Finished implementation of base model training and evaluation. Changed the specific classes to generic positive negative and neutral classes to reduce number of possible output classes for prediction.

7th March:
Implemented inferrence on one image. Tested the model to check if emotion detector is working on real and unseen data. 

15th March:
Started work on post processing of outputs of the model classifier, retrieved outputs as a dictionary and thinking of format of data to be outputted using MQTT

22nd March:
Completed post processing of outputs, decided to send data in the form of 
(18, 31, 25): {0: 'Neutral'}
(18, 31, 27): {0: 'Neutral'}
(18, 31, 28): {0: 'Neutral', 1: 'Negative'}
where {time_stamp : {user:emotion}}


1st April:
Completed Integration of MQTT into the system, able to receive from end device and transmitted from the pi.

6th April
Tested full integration of system along with the SER, along with the display of results onto the GUI and integration with the real video.